{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5uV6kTfRB+F0F53L9JnUQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QBxyWA7Hhc1b","executionInfo":{"status":"ok","timestamp":1734333816653,"user_tz":300,"elapsed":18318,"user":{"displayName":"Onkar Prasanna Kher","userId":"05308220019853618953"}},"outputId":"ea573b44-457a-4eec-8199-7b40f2e5a7c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","# Mounting the Google Drive to save processed files\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Function to load and concatenate the datasets\n","def load_and_concat_data(file_paths):\n","    \"\"\"\n","    Load and concatenate datasets from the given file paths.\n","    :param file_paths: List of file paths for datasets\n","    :return: Concatenated features (X) and targets (y)\n","    \"\"\"\n","    data = [pd.read_csv(file, header=None) for file in file_paths]\n","    combined_data = pd.concat(data, axis=0).reset_index(drop=True)\n","    # Features: In all the columns except for the last two\n","    X = combined_data.iloc[:, :-2]\n","    # Targets: In the last two columns\n","    y = combined_data.iloc[:, -2:]\n","    return X, y\n","\n","# Function to preprocess the data\n","def preprocess_data(X_train, X_val, X_test):\n","    \"\"\"\n","    Normalize the datasets using StandardScaler.\n","    :param X_train: Training features\n","    :param X_val: Validation features\n","    :param X_test: Test features\n","    :return: Scaled datasets and the scaler object\n","    \"\"\"\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_val_scaled = scaler.transform(X_val)\n","    X_test_scaled = scaler.transform(X_test)\n","    return X_train_scaled, X_val_scaled, X_test_scaled, scaler"],"metadata":{"id":"k7l7SD6fhiyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Function to load and concatenate data from multiple files\n","def load_and_concat_data(file_paths):\n","    X_list, y_list = [], []\n","    for file in file_paths:\n","        data = pd.read_csv(file, header=None)\n","        data = data.apply(pd.to_numeric, errors='coerce').fillna(data.mean())  # Handling all the non-numeric and NaN values\n","        X_list.append(data.iloc[:, :-2].values)\n","        y_list.append(data.iloc[:, -2:].values)\n","    X = np.vstack(X_list)\n","    y = np.vstack(y_list)\n","    return X, y\n","\n","# Function to preprocess and scale the data\n","def preprocess_data(X_train, X_val, X_test):\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train)\n","    X_val_scaled = scaler.transform(X_val)\n","    X_test_scaled = scaler.transform(X_test)\n","    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n","\n","# Make sure to change the paths as per your file directory\n","# File paths for the training and the test datasets\n","train_files = [\n","    \"/content/drive/MyDrive/ML/Train/Aug14_Box_g17.csv\",\n","    \"/content/drive/MyDrive/ML/Train/July22_23.csv\",\n","    \"/content/drive/MyDrive/ML/Train/July28_Special_2.csv\"\n","]\n","test_files = [\n","    \"/content/drive/MyDrive/ML/Test/Aug14_Box_g11.csv\",\n","    \"/content/drive/MyDrive/ML/Test/July22_68.csv\"\n","]\n","\n","# Loading and concatenating the datasets\n","X_train_full, y_train_full = load_and_concat_data(train_files)\n","X_test_full, y_test_full = load_and_concat_data(test_files)\n","\n","# Shuffling the data before splitting\n","shuffle_indices = np.random.permutation(len(X_train_full))\n","X_train_full = X_train_full[shuffle_indices]\n","y_train_full = y_train_full[shuffle_indices]\n","\n","# Splitting the training data into train and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, shuffle=False)\n","\n","# Preprocessing the data\n","X_train_scaled, X_val_scaled, X_test_scaled, scaler = preprocess_data(X_train, X_val, X_test_full)\n","\n","# Printing out the data shapes for verification\n","print(f\"Training features: {X_train_scaled.shape}, Training targets: {y_train.shape}\")\n","print(f\"Validation features: {X_val_scaled.shape}, Validation targets: {y_val.shape}\")\n","print(f\"Test features: {X_test_scaled.shape}, Test targets: {y_test_full.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WU6Cvd8htq_","executionInfo":{"status":"ok","timestamp":1734333856884,"user_tz":300,"elapsed":39142,"user":{"displayName":"Onkar Prasanna Kher","userId":"05308220019853618953"}},"outputId":"2d70e2ef-fac0-40aa-d875-9624577dced2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training features: (56289, 1092), Training targets: (56289, 2)\n","Validation features: (14073, 1092), Validation targets: (14073, 2)\n","Test features: (39804, 1092), Test targets: (39804, 2)\n"]}]},{"cell_type":"code","source":["# Saving the scaled datasets\n","np.save(save_dir + \"X_train_scaled.npy\", X_train_scaled)\n","np.save(save_dir + \"X_val_scaled.npy\", X_val_scaled)\n","np.save(save_dir + \"X_test_scaled.npy\", X_test_scaled)\n","\n","# Saving the target arrays (already NumPy arrays, no need for .to_numpy())\n","np.save(save_dir + \"y_train.npy\", y_train)\n","np.save(save_dir + \"y_val.npy\", y_val)\n","np.save(save_dir + \"y_test.npy\", y_test_full)"],"metadata":{"id":"-ESMTYSbhv6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Listing  the saved files in the Google Drive directory\n","import os\n","files = os.listdir(save_dir)\n","print(\"Saved files in Google Drive:\")\n","print(files)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWG_oLpthx-e","executionInfo":{"status":"ok","timestamp":1734320094985,"user_tz":300,"elapsed":3,"user":{"displayName":"Onkar Prasanna Kher","userId":"05308220019853618953"}},"outputId":"b80205d0-de21-4f3d-f79d-dfb63b358c3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved files in Google Drive:\n","['X_train_scaled.npy', 'X_val_scaled.npy', 'X_test_scaled.npy', 'y_train.npy', 'y_val.npy', 'y_test.npy', 'scaler.pkl']\n"]}]}]}